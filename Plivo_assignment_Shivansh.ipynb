{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66sPLv6BrDut",
        "outputId": "8d84f598-9d15-48fe-fccf-944c0ba027f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Collecting seqeval\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/43.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting faker\n",
            "  Downloading faker-38.2.0-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: tzdata in /usr/local/lib/python3.12/dist-packages (from faker) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading faker-38.2.0-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m44.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16162 sha256=084664745040132d0411f077d1b7fff789899fd75f3e0cdc04f60ad894535271\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/b8/73/0b2c1a76b701a677653dd79ece07cfabd7457989dbfbdcd8d7\n",
            "Successfully built seqeval\n",
            "Installing collected packages: faker, seqeval\n",
            "Successfully installed faker-38.2.0 seqeval-1.2.2\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers datasets seqeval faker numpy scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d58782a4"
      },
      "source": [
        "To load files from Google Drive, you first need to mount your Drive to this Colab notebook. This will allow you to access your files as if they were on the local file system."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aeef9bd4",
        "outputId": "ab506778-0593-4300-f110-d2fad05d0e7a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32d90ccd"
      },
      "source": [
        "After running the cell above and following the authentication steps, your Google Drive will be mounted at `/content/drive`. You can then navigate to your files. For example, to list the contents of your Drive's root directory, you can use:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5a85e602",
        "outputId": "46875723-73b5-44de-885f-3ede217dc25b"
      },
      "source": [
        "import os\n",
        "\n",
        "os.chdir('/content/drive/MyDrive/pii_ner_assignment_IITB')\n",
        "print(os.getcwd())"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/pii_ner_assignment_IITB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5e784720",
        "outputId": "c261e499-51fd-4079-f31f-a3f486c76192"
      },
      "source": [
        "import os\n",
        "\n",
        "files_in_pwd = os.listdir('.')\n",
        "print(files_in_pwd)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['assignment.md', 'requirements.txt', 'README.md', 'data', 'src', 'out']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fec1fea"
      },
      "source": [
        "## Setup and Quick Baseline\n",
        "\n",
        "### Subtask:\n",
        "Install dependencies, train a baseline model, predict on dev and stress sets, and evaluate the performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7807a84",
        "outputId": "e74c79d8-5dac-441b-d7a5-d4b5fb9b5c64"
      },
      "source": [
        "# Check if requirements.txt exists before installing\n",
        "if os.path.exists('requirements.txt'):\n",
        "    !pip install -r requirements.txt\n",
        "else:\n",
        "    print('requirements.txt not found. Please ensure it is in the current working directory.')\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 1)) (2.9.0+cu126)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 2)) (4.57.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 3)) (2.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 4)) (4.67.1)\n",
            "Requirement already satisfied: seqeval in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 5)) (1.2.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (3.5.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers->-r requirements.txt (line 2)) (0.36.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers->-r requirements.txt (line 2)) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers->-r requirements.txt (line 2)) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers->-r requirements.txt (line 2)) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers->-r requirements.txt (line 2)) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers->-r requirements.txt (line 2)) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers->-r requirements.txt (line 2)) (0.7.0)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.12/dist-packages (from seqeval->-r requirements.txt (line 5)) (1.6.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers->-r requirements.txt (line 2)) (1.2.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.21.3->seqeval->-r requirements.txt (line 5)) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.21.3->seqeval->-r requirements.txt (line 5)) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.21.3->seqeval->-r requirements.txt (line 5)) (3.6.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->-r requirements.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->-r requirements.txt (line 1)) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers->-r requirements.txt (line 2)) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers->-r requirements.txt (line 2)) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers->-r requirements.txt (line 2)) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers->-r requirements.txt (line 2)) (2025.11.12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7fec1ac"
      },
      "source": [
        "**Reasoning**:\n",
        "The next step is to train a baseline model using the provided `train.py` script, as specified in the assignment's suggested workflow. This will create the `out` directory and save the trained model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the output directory if it doesn't exist\n",
        "output_dir = 'out'\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n"
      ],
      "metadata": {
        "id": "iQMC_scjxb0e"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating random data for Dev set"
      ],
      "metadata": {
        "id": "vyZeKm-t8owl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import random\n",
        "from faker import Faker\n",
        "from datetime import datetime\n",
        "\n",
        "fake = Faker()\n",
        "\n",
        "# 1. HARDCODED \"SPOKEN\" ARTIFACTS\n",
        "# STT often transcribes these specific ways\n",
        "INDIAN_NAMES = [\n",
        "    \"ramesh\", \"suresh\", \"amit\", \"priya\", \"rahul\", \"sharma\", \"patel\", \"singh\",\n",
        "    \"kumar\", \"aditya\", \"sneha\", \"rohit\", \"vikram\", \"anjali\", \"deepak\", \"neha\",\n",
        "    \"gupta\", \"verma\", \"reddy\", \"nair\", \"khan\", \"mishra\", \"joshi\"\n",
        "]\n",
        "\n",
        "DIGIT_MAP = {\n",
        "    \"0\": \"zero\", \"1\": \"one\", \"2\": \"two\", \"3\": \"three\", \"4\": \"four\",\n",
        "    \"5\": \"five\", \"6\": \"six\", \"7\": \"seven\", \"8\": \"eight\", \"9\": \"nine\"\n",
        "}\n",
        "\n",
        "MONTHS = [\n",
        "    \"january\", \"february\", \"march\", \"april\", \"may\", \"june\",\n",
        "    \"july\", \"august\", \"september\", \"october\", \"november\", \"december\"\n",
        "]\n",
        "\n",
        "# 2. HELPER FUNCTIONS FOR \"SPOKEN\" CONVERSION\n",
        "def to_spoken_digits(text):\n",
        "    \"\"\"Converts '9820' -> 'nine eight two zero'\"\"\"\n",
        "    out = []\n",
        "    for char in text:\n",
        "        if char in DIGIT_MAP:\n",
        "            out.append(DIGIT_MAP[char])\n",
        "        else:\n",
        "            out.append(char)\n",
        "    return \" \".join(out)\n",
        "\n",
        "def get_spoken_date():\n",
        "    \"\"\"Generates 'january first', '10th of may', 'march 23'\"\"\"\n",
        "    # Random date object\n",
        "    d = fake.date_object()\n",
        "    day = d.day\n",
        "    month = MONTHS[d.month - 1]\n",
        "    year = d.year\n",
        "\n",
        "    style = random.choice([1, 2, 3, 4])\n",
        "\n",
        "    if style == 1:\n",
        "        return f\"{month} {day}\"  # \"march 12\"\n",
        "    elif style == 2:\n",
        "        return f\"{day} of {month}\" # \"12 of march\"\n",
        "    elif style == 3:\n",
        "        # Ordinal approximate (stt often misses this, but useful)\n",
        "        suffix = \"th\"\n",
        "        if day in [1, 21, 31]: suffix = \"st\"\n",
        "        elif day in [2, 22]: suffix = \"nd\"\n",
        "        elif day in [3, 23]: suffix = \"rd\"\n",
        "        return f\"{month} {day}{suffix}\" # \"march 1st\"\n",
        "    else:\n",
        "        return f\"{month} {day} {year}\" # \"march 12 2022\"\n",
        "\n",
        "def make_email_spoken(email):\n",
        "    \"\"\"Converts 'bob@gmail.com' -> 'bob at gmail dot com'\"\"\"\n",
        "    return email.replace(\"@\", \" at \").replace(\".\", \" dot \")\n",
        "\n",
        "# 3. GENERATOR\n",
        "def generate_v4_example(uid):\n",
        "    templates = [\n",
        "        # TEMPLATE PATTERNS (Subject + Verb + Entity)\n",
        "        (\"my credit card is {CREDIT_CARD}\", \"CREDIT_CARD\"),\n",
        "        (\"card number {CREDIT_CARD}\", \"CREDIT_CARD\"),\n",
        "        (\"{CREDIT_CARD} is my card\", \"CREDIT_CARD\"),\n",
        "\n",
        "        (\"call me at {PHONE}\", \"PHONE\"),\n",
        "        (\"phone number is {PHONE}\", \"PHONE\"),\n",
        "        (\"dial {PHONE}\", \"PHONE\"),\n",
        "\n",
        "        (\"email address is {EMAIL}\", \"EMAIL\"),\n",
        "        (\"contact {EMAIL}\", \"EMAIL\"),\n",
        "        (\"mail to {EMAIL}\", \"EMAIL\"),\n",
        "\n",
        "        (\"my name is {PERSON_NAME}\", \"PERSON_NAME\"),\n",
        "        (\"this is {PERSON_NAME}\", \"PERSON_NAME\"),\n",
        "        (\"i am {PERSON_NAME}\", \"PERSON_NAME\"),\n",
        "\n",
        "        (\"date of birth {DATE}\", \"DATE\"),\n",
        "        (\"meeting on {DATE}\", \"DATE\"),\n",
        "        (\"scheduled for {DATE}\", \"DATE\"),\n",
        "        (\"today is {DATE}\", \"DATE\"),\n",
        "\n",
        "        (\"live in {CITY}\", \"CITY\"),\n",
        "        (\"from {CITY}\", \"CITY\"),\n",
        "        (\"visit {LOCATION}\", \"LOCATION\"),\n",
        "\n",
        "        # MULTI-ENTITY\n",
        "        (\"name {PERSON_NAME} card {CREDIT_CARD}\", [\"PERSON_NAME\", \"CREDIT_CARD\"]),\n",
        "        (\"call {PERSON_NAME} at {PHONE}\", [\"PERSON_NAME\", \"PHONE\"]),\n",
        "    ]\n",
        "\n",
        "    # 20% Negative Examples (No PII) to fix Precision\n",
        "    if random.random() < 0.2:\n",
        "        text = fake.sentence()\n",
        "        # Remove punctuation to look like STT\n",
        "        text = text.replace(\".\", \"\").replace(\",\", \"\").lower()\n",
        "        return {\"id\": f\"train_{uid}\", \"text\": text, \"entities\": []}\n",
        "\n",
        "    template, ent_types = random.choice(templates)\n",
        "    if not isinstance(ent_types, list): ent_types = [ent_types]\n",
        "\n",
        "    entity_data = {}\n",
        "    for et in ent_types:\n",
        "        if et == \"CREDIT_CARD\":\n",
        "            val = fake.credit_card_number()\n",
        "            # 80% Spoken digits (Stress set is heavy on this)\n",
        "            if random.random() < 0.8:\n",
        "                val = to_spoken_digits(val)\n",
        "            entity_data[et] = val\n",
        "\n",
        "        elif et == \"PHONE\":\n",
        "            val = fake.phone_number()\n",
        "            # 80% Spoken digits\n",
        "            if random.random() < 0.8:\n",
        "                val = to_spoken_digits(val)\n",
        "            else:\n",
        "                val = val.replace(\"-\", \" \").replace(\"(\", \"\").replace(\")\", \"\")\n",
        "            entity_data[et] = val\n",
        "\n",
        "        elif et == \"EMAIL\":\n",
        "            # 90% Spoken emails (Stress set is heavy on this)\n",
        "            val = fake.email()\n",
        "            if random.random() < 0.9:\n",
        "                val = make_email_spoken(val)\n",
        "            entity_data[et] = val\n",
        "\n",
        "        elif et == \"PERSON_NAME\":\n",
        "            # 50% Indian Names\n",
        "            if random.random() < 0.5:\n",
        "                # Create full name \"Ramesh Kumar\"\n",
        "                val = f\"{random.choice(INDIAN_NAMES)} {random.choice(INDIAN_NAMES)}\"\n",
        "            else:\n",
        "                val = fake.name()\n",
        "            entity_data[et] = val\n",
        "\n",
        "        elif et == \"DATE\":\n",
        "            # 100% Spoken Dates (CRITICAL FIX for your 0.00 score)\n",
        "            entity_data[et] = get_spoken_date()\n",
        "\n",
        "        elif et == \"CITY\": entity_data[et] = fake.city()\n",
        "        elif et == \"LOCATION\": entity_data[et] = fake.address()\n",
        "\n",
        "    # Build Text\n",
        "    full_text = \"\"\n",
        "    spans = []\n",
        "    parts = template.split(\"{\")\n",
        "\n",
        "    for part in parts:\n",
        "        if \"}\" in part:\n",
        "            etype, rest = part.split(\"}\")\n",
        "            val = str(entity_data[etype]).lower()\n",
        "\n",
        "            # STT Noise: Randomly remove spaces in digit sequences\n",
        "            if etype in [\"CREDIT_CARD\", \"PHONE\"] and random.random() < 0.3:\n",
        "                val = val.replace(\" \", \"\")\n",
        "\n",
        "            start = len(full_text)\n",
        "            full_text += val\n",
        "            end = len(full_text)\n",
        "\n",
        "            spans.append({\"start\": start, \"end\": end, \"label\": etype})\n",
        "            full_text += rest.lower()\n",
        "        else:\n",
        "            full_text += part.lower()\n",
        "\n",
        "    return {\n",
        "        \"id\": f\"train_{uid}\",\n",
        "        \"text\": full_text,\n",
        "        \"entities\": spans\n",
        "    }\n",
        "\n",
        "# GENERATE 3000 EXAMPLES (Need volume to learn patterns)\n",
        "print(\"Generating V4 'Spoken' Data...\")\n",
        "with open(\"data/train.jsonl\", \"w\") as f:\n",
        "    for i in range(3000):\n",
        "        f.write(json.dumps(generate_v4_example(i)) + \"\\n\")\n",
        "\n",
        "with open(\"data/dev.jsonl\", \"w\") as f:\n",
        "    for i in range(400):\n",
        "        f.write(json.dumps(generate_v4_example(i + 10000)) + \"\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jgWYXa-8vrIx",
        "outputId": "f620935e-fdf8-4d35-f092-4327d91c475c"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating V4 'Spoken' Data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python src/train.py \\\n",
        "  --model_name distilroberta-base \\\n",
        "  --train data/train.jsonl \\\n",
        "  --dev data/dev.jsonl \\\n",
        "  --out_dir out \\\n",
        "  --epochs 5 \\\n",
        "  --batch_size 16 \\\n",
        "  --lr 3e-5 \\\n",
        "  --max_length 128 \\\n",
        "  --freeze_layers 3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4fjjDttwRK7",
        "outputId": "696ca3ee-3eee-4e5b-93da-894b665912fc"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading tokenizer: distilroberta-base\n",
            "Loading training data from data/train.jsonl...\n",
            "Loading dev data from data/dev.jsonl...\n",
            "2025-11-25 12:25:58.500495: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764073558.559859   17377 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764073558.569219   17377 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764073558.604719   17377 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764073558.604761   17377 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764073558.604769   17377 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764073558.604774   17377 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-25 12:25:58.611567: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Freezing embeddings and first 3 layers...\n",
            "Detected RoBERTa architecture.\n",
            "Calculating class weights...\n",
            "Class Weights applied: tensor([ 1.0000,  5.7915,  1.0000,  5.4635,  1.0000,  7.4858,  1.1175,  4.4073,\n",
            "         1.3226,  5.6227,  1.9730, 10.0000,  4.0776, 10.0000,  1.4330],\n",
            "       device='cuda:0')\n",
            "Starting training...\n",
            "Epoch 1/5: 100% 188/188 [00:11<00:00, 16.57it/s, loss=0.1381]\n",
            "Epoch 1 | Train Loss: 1.0740 | Val Loss: 0.0400\n",
            "  -> New best model saved (Val Loss: 0.0400)\n",
            "Epoch 2/5: 100% 188/188 [00:06<00:00, 28.25it/s, loss=0.0151]\n",
            "Epoch 2 | Train Loss: 0.0421 | Val Loss: 0.0092\n",
            "  -> New best model saved (Val Loss: 0.0092)\n",
            "Epoch 3/5: 100% 188/188 [00:05<00:00, 34.62it/s, loss=0.0211]\n",
            "Epoch 3 | Train Loss: 0.0154 | Val Loss: 0.0045\n",
            "  -> New best model saved (Val Loss: 0.0045)\n",
            "Epoch 4/5: 100% 188/188 [00:05<00:00, 32.85it/s, loss=0.0102]\n",
            "Epoch 4 | Train Loss: 0.0110 | Val Loss: 0.0033\n",
            "  -> New best model saved (Val Loss: 0.0033)\n",
            "Epoch 5/5: 100% 188/188 [00:05<00:00, 33.70it/s, loss=0.0046]\n",
            "Epoch 5 | Train Loss: 0.0092 | Val Loss: 0.0033\n",
            "  -> New best model saved (Val Loss: 0.0033)\n",
            "Training complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python src/measure_latency.py \\\n",
        "  --model_dir out \\\n",
        "  --input data/dev.jsonl \\\n",
        "  --max_length 128 \\\n",
        "  --runs 100\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I9tDwrDPx9dk",
        "outputId": "e4efb44c-c2bb-4f98-9445-62637cfde13e"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-11-25 12:27:17.603570: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764073637.632457   17811 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764073637.641388   17811 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764073637.666100   17811 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764073637.666131   17811 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764073637.666138   17811 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764073637.666148   17811 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-25 12:27:17.672978: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Latency over 100 runs (batch_size=1):\n",
            "  p50: 5.34 ms\n",
            "  p95: 7.20 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python src/predict.py \\\n",
        "  --model_dir out \\\n",
        "  --input data/dev.jsonl \\\n",
        "  --output out/dev_pred.json \\\n",
        "  --max_length 128\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4UnhxxhwzZ8I",
        "outputId": "08ca6189-76a3-456f-8696-ab615524a073"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-11-25 12:27:33.468190: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764073653.485046   17907 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764073653.490271   17907 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764073653.506764   17907 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764073653.506794   17907 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764073653.506799   17907 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764073653.506805   17907 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-25 12:27:33.512209: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Wrote predictions for 400 utterances to out/dev_pred.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python src/eval_span_f1.py \\\n",
        "  --gold data/dev.jsonl \\\n",
        "  --pred out/dev_pred.json\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U7aRtYDtzZ4q",
        "outputId": "4c75589e-8d04-4b1b-a5f1-2ba2f1bcc593"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Per-entity metrics:\n",
            "CITY            P=0.966 R=1.000 F1=0.982\n",
            "CREDIT_CARD     P=1.000 R=1.000 F1=1.000\n",
            "DATE            P=1.000 R=1.000 F1=1.000\n",
            "EMAIL           P=0.955 R=0.977 F1=0.966\n",
            "LOCATION        P=1.000 R=1.000 F1=1.000\n",
            "PERSON_NAME     P=1.000 R=1.000 F1=1.000\n",
            "PHONE           P=1.000 R=1.000 F1=1.000\n",
            "\n",
            "Macro-F1: 0.993\n",
            "\n",
            "PII-only metrics: P=0.993 R=0.997 F1=0.995\n",
            "Non-PII metrics: P=0.978 R=1.000 F1=0.989\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prediction and evaluation on the **Stress Set**"
      ],
      "metadata": {
        "id": "edRyKOG01m4q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Predict on Stress Set\n",
        "!python src/predict.py \\\n",
        "  --model_dir out \\\n",
        "  --input data/stress.jsonl \\\n",
        "  --output out/stress_pred.json \\\n",
        "  --max_length 128\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLouCAYj0l0I",
        "outputId": "02559dd7-aa34-4145-b3c5-a8260ed252fc"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-11-25 12:27:50.696156: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764073670.713683   17992 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764073670.719011   17992 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764073670.741449   17992 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764073670.741484   17992 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764073670.741488   17992 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764073670.741492   17992 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-25 12:27:50.748681: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Wrote predictions for 100 utterances to out/stress_pred.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Evaluate Stress Set\n",
        "!python src/eval_span_f1.py \\\n",
        "  --gold data/stress.jsonl \\\n",
        "  --pred out/stress_pred.json\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UUo254DO1klw",
        "outputId": "d0e656d7-2576-473c-ebf4-be432b38becb"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Per-entity metrics:\n",
            "CITY            P=0.857 R=0.750 F1=0.800\n",
            "CREDIT_CARD     P=0.277 R=0.450 F1=0.343\n",
            "DATE            P=0.952 R=1.000 F1=0.976\n",
            "EMAIL           P=0.000 R=0.000 F1=0.000\n",
            "PERSON_NAME     P=0.262 R=0.975 F1=0.413\n",
            "PHONE           P=0.257 R=0.450 F1=0.327\n",
            "\n",
            "Macro-F1: 0.476\n",
            "\n",
            "PII-only metrics: P=0.368 R=0.730 F1=0.489\n",
            "Non-PII metrics: P=0.857 R=0.750 F1=0.800\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9Mtb-8Ci1xih"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}